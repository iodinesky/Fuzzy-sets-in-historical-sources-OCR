{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g-6llNlDlPO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from russpelling import *\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spaczz.matcher import FuzzyMatcher\n",
        "import enchant\n",
        "from statistics import mean\n",
        "\n",
        "dictionary = enchant.Dict(\"ru_RU\")\n",
        "\n",
        "#Директория с файлами распознанных источников \n",
        "os.chdir('C:/Users/YOUR CODE HERE')\n",
        "\n",
        "\n",
        "list_of_files = os.listdir()\n",
        "\n",
        "\n",
        "def listmerge3(lstlst):\n",
        "    all = []\n",
        "    for lst in lstlst:\n",
        "        all.extend(lst)\n",
        "    return all\n",
        "\n",
        "\n",
        "# очистка текста от лишних символов html-формата\n",
        "\n",
        "Collection_of_LDA_words_counters_for_text = []\n",
        "Collection_of_FUZZED_words_counters_for_text = []\n",
        "Collection_of_recognized_percents = []\n",
        "\n",
        "for file in list_of_files:\n",
        "    if file in [\"adj-with-ija.txt\", \"fuzzy_patterns.txt\", \"ID_patterns_rebuild_1.csv\"]:\n",
        "        continue\n",
        "    else:\n",
        "        f = open(file, encoding='utf-8', mode='r')\n",
        "        html = f.read()\n",
        "        soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.extract()\n",
        "\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "\n",
        "        text_old = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "        text_old = text_old.split('\\n')\n",
        "\n",
        "        text_old = [text.split(' ') for text in text_old]\n",
        "\n",
        "        text_old = listmerge3(text_old)\n",
        "\n",
        "        text_new_orth = [normalize(word) for word in text_old]\n",
        "\n",
        "        text_new_orth = [word.strip('.') for word in text_new_orth]\n",
        "        text_new_orth = [word.strip('_') for word in text_new_orth]\n",
        "        text_new_orth = [word.strip(',') for word in text_new_orth]\n",
        "        text_new_orth = [word for word in text_new_orth if word != '']\n",
        "\n",
        "        # загрузка предобученной модели SpaCy\n",
        "\n",
        "        nlp = spacy.load(\"ru_core_news_sm\")\n",
        "        matcher = FuzzyMatcher(nlp.vocab)\n",
        "\n",
        "        # чтение паттернов Fuzzy-Match, подготовка небольшого датасета\n",
        "\n",
        "        f = open('fuzzy_patterns.txt', encoding='utf-8', mode='r')\n",
        "        text_a = f.read()\n",
        "        data = text_a.split('____________________')\n",
        "        data = [elem.replace('\\n', ' ') for elem in data]\n",
        "        data = [elem.split(' ') for elem in data]\n",
        "        data = [set(elem) for elem in data]\n",
        "        list_of_patterns = []\n",
        "        for elem in data:\n",
        "            elem = list(elem)\n",
        "            elem.pop(0)\n",
        "            list_of_patterns.append(elem)\n",
        "\n",
        "        # импорт таблицы с ID для паттернов (необходимо для дальнейшего восстановления слов по ним)\n",
        "\n",
        "        df = pd.read_csv('ID_patterns_rebuild_1.csv', encoding=\"cp1251\", delimiter=';')\n",
        "\n",
        "        df['Формы'] = list_of_patterns\n",
        "\n",
        "        # добавление наших паттернов в шаблоны Fuzzy-Matcher'a\n",
        "\n",
        "        for i in range(0, len(list_of_patterns)):\n",
        "            pattern_forms = [nlp(word) for word in list_of_patterns[i]]\n",
        "            matcher.add(str(i), pattern_forms, kwargs=[{\"min_r2\": 80}])\n",
        "\n",
        "        matcher.add('31', [nlp(\"проц\")], kwargs=[{\"min_r2\": 80}])\n",
        "        df.loc[len(df.index)] = [31, 'проц', ['проц']]\n",
        "\n",
        "        # проверка слов на орфографические ошибки, сравнение не прошедших проверку слов с паттернами Fuzzy-match\n",
        "\n",
        "        word_counter = 0\n",
        "        words_to_pop_indx = []\n",
        "        for word in text_new_orth:\n",
        "            check = dictionary.check(word)\n",
        "            doc = nlp(word)\n",
        "            if check == False:\n",
        "                match_operation = matcher(doc)\n",
        "                if len(match_operation) == 0:\n",
        "                    words_to_pop_indx.append(word_counter)\n",
        "                else:\n",
        "                    text_new_orth[word_counter] = df.at[int(match_operation[0][0]), \"Паттерн\"]\n",
        "\n",
        "            word_counter += 1\n",
        "\n",
        "        text_new_orth_FUZZ_filter = [text_new_orth[i] for i in range(0, len(text_new_orth)) if i not in words_to_pop_indx]\n",
        "\n",
        "        final_text = ' '.join(text_new_orth_FUZZ_filter)\n",
        "\n",
        "\n",
        "        # сохранение файлов в новую директорию\n",
        "\n",
        "\n",
        "        file_path = \"C:/Users/YOUR CODE HERE/\" + file[:-4] + '.txt'\n",
        "\n",
        "        with open(file_path, \"w\") as f_1:\n",
        "            f_1.write(final_text)\n",
        "            f_1.close()\n",
        "\n",
        "        f.close()\n"
      ]
    }
  ]
}